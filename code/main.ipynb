{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMm+TsxWtjByOO2JL4zx9R3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamranz/ProcessSequencePrediction/blob/master/code/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ8KsxiLYwZy",
        "outputId": "4be660b1-10c7-4e80-d44f-c4ba84b1df65"
      },
      "source": [
        "!git clone https://github.com/verenich/ProcessSequencePrediction"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ProcessSequencePrediction'...\n",
            "remote: Enumerating objects: 273, done.\u001b[K\n",
            "remote: Total 273 (delta 0), reused 0 (delta 0), pack-reused 273\u001b[K\n",
            "Receiving objects: 100% (273/273), 3.57 MiB | 2.89 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FstsQj_rZ1Lf",
        "outputId": "1f0045be-d7cd-4522-edc1-2e9ba5a12f55"
      },
      "source": [
        "!pip install keras tensorflow unicodecsv numpy jellyfish sklearn matplotlib h5py more-itertools"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: unicodecsv in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.6/dist-packages (0.8.2)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (8.6.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "L4C3g4DGZWWe",
        "outputId": "ab81047c-b3ff-4337-87f9-34109ba88ba8"
      },
      "source": [
        "'''\r\n",
        "this script trains an LSTM model on one of the data files in the data folder of\r\n",
        "this repository. the input file can be changed to another file from the data folder\r\n",
        "by changing its name in line 46.\r\n",
        "\r\n",
        "it is recommended to run this script on GPU, as recurrent networks are quite \r\n",
        "computationally intensive.\r\n",
        "\r\n",
        "Author: Niek Tax\r\n",
        "'''\r\n",
        "\r\n",
        "from __future__ import print_function, division\r\n",
        "from keras.models import Sequential, Model\r\n",
        "from keras.layers.core import Dense\r\n",
        "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\r\n",
        "from keras.layers import Input\r\n",
        "from keras.utils.data_utils import get_file\r\n",
        "from keras.optimizers import Nadam\r\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\r\n",
        "from keras.layers.normalization import BatchNormalization\r\n",
        "from collections import Counter\r\n",
        "import unicodecsv\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import sys\r\n",
        "import os\r\n",
        "import copy\r\n",
        "import csv\r\n",
        "import time\r\n",
        "from datetime import datetime\r\n",
        "from math import log\r\n",
        "\r\n",
        "izip = zip\r\n",
        "\r\n",
        "eventlog = \"helpdesk.csv\"\r\n",
        "\r\n",
        "########################################################################################\r\n",
        "#\r\n",
        "# this part of the code opens the file, reads it into three following variables\r\n",
        "#\r\n",
        "\r\n",
        "lines = [] #these are all the activity seq\r\n",
        "timeseqs = [] #time sequences (differences between two events)\r\n",
        "timeseqs2 = [] #time sequences (differences between the current and first)\r\n",
        "\r\n",
        "#helper variables\r\n",
        "lastcase = ''\r\n",
        "line = ''\r\n",
        "firstLine = True\r\n",
        "times = []\r\n",
        "times2 = []\r\n",
        "numlines = 0\r\n",
        "casestarttime = None\r\n",
        "lasteventtime = None\r\n",
        "\r\n",
        "\r\n",
        "csvfile = open('../data/%s' % eventlog, 'r')\r\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\r\n",
        "next(spamreader, None)  # skip the headers\r\n",
        "ascii_offset = 161\r\n",
        "\r\n",
        "for row in spamreader: #the rows are \"CaseID,ActivityID,CompleteTimestamp\"\r\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\") #creates a datetime object from row[2]\r\n",
        "    if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\r\n",
        "        casestarttime = t\r\n",
        "        lasteventtime = t\r\n",
        "        lastcase = row[0]\r\n",
        "        if not firstLine:\r\n",
        "            lines.append(line)\r\n",
        "            timeseqs.append(times)\r\n",
        "            timeseqs2.append(times2)\r\n",
        "        line = ''\r\n",
        "        times = []\r\n",
        "        times2 = []\r\n",
        "        numlines+=1\r\n",
        "    line+=unichr(int(row[1])+ascii_offset)\r\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\r\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\r\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\r\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\r\n",
        "    times.append(timediff)\r\n",
        "    times2.append(timediff2)\r\n",
        "    lasteventtime = t\r\n",
        "    firstLine = False\r\n",
        "\r\n",
        "# add last case\r\n",
        "lines.append(line)\r\n",
        "timeseqs.append(times)\r\n",
        "timeseqs2.append(times2)\r\n",
        "numlines+=1\r\n",
        "\r\n",
        "########################################\r\n",
        "\r\n",
        "divisor = np.mean([item for sublist in timeseqs for item in sublist]) #average time between events\r\n",
        "print('divisor: {}'.format(divisor))\r\n",
        "divisor2 = np.mean([item for sublist in timeseqs2 for item in sublist]) #average time between current and first events\r\n",
        "print('divisor2: {}'.format(divisor2))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#########################################################################################################\r\n",
        "\r\n",
        "# separate training data into 3 parts\r\n",
        "\r\n",
        "elems_per_fold = int(round(numlines/3))\r\n",
        "fold1 = lines[:elems_per_fold]\r\n",
        "fold1_t = timeseqs[:elems_per_fold]\r\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\r\n",
        "\r\n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\r\n",
        "\r\n",
        "fold3 = lines[2*elems_per_fold:]\r\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\r\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\r\n",
        "\r\n",
        "#leave away fold3 for now\r\n",
        "lines = fold1 + fold2\r\n",
        "lines_t = fold1_t + fold2_t\r\n",
        "lines_t2 = fold1_t2 + fold2_t2\r\n",
        "\r\n",
        "step = 1\r\n",
        "sentences = []\r\n",
        "softness = 0\r\n",
        "next_chars = []\r\n",
        "lines = map(lambda x: x+'!',lines) #put delimiter symbol\r\n",
        "maxlen = max(map(lambda x: len(x),lines)) #find maximum line size\r\n",
        "\r\n",
        "# next lines here to get all possible characters for events and annotate them with numbers\r\n",
        "chars = map(lambda x: set(x),lines)\r\n",
        "chars = list(set().union(*chars))\r\n",
        "chars.sort()\r\n",
        "target_chars = copy.copy(chars)\r\n",
        "chars.remove('!')\r\n",
        "print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))\r\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))\r\n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))\r\n",
        "target_char_indices = dict((c, i) for i, c in enumerate(target_chars))\r\n",
        "target_indices_char = dict((i, c) for i, c in enumerate(target_chars))\r\n",
        "print(indices_char)\r\n",
        "\r\n",
        "\r\n",
        "csvfile = open('../data/%s' % eventlog, 'r')\r\n",
        "spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\r\n",
        "next(spamreader, None)  # skip the headers\r\n",
        "lastcase = ''\r\n",
        "line = ''\r\n",
        "firstLine = True\r\n",
        "lines = []\r\n",
        "timeseqs = []\r\n",
        "timeseqs2 = []\r\n",
        "timeseqs3 = []\r\n",
        "timeseqs4 = []\r\n",
        "times = []\r\n",
        "times2 = []\r\n",
        "times3 = []\r\n",
        "times4 = []\r\n",
        "numlines = 0\r\n",
        "casestarttime = None\r\n",
        "lasteventtime = None\r\n",
        "for row in spamreader:\r\n",
        "    t = time.strptime(row[2], \"%Y-%m-%d %H:%M:%S\")\r\n",
        "    if row[0]!=lastcase:\r\n",
        "        casestarttime = t\r\n",
        "        lasteventtime = t\r\n",
        "        lastcase = row[0]\r\n",
        "        if not firstLine:\r\n",
        "            lines.append(line)\r\n",
        "            timeseqs.append(times)\r\n",
        "            timeseqs2.append(times2)\r\n",
        "            timeseqs3.append(times3)\r\n",
        "            timeseqs4.append(times4)\r\n",
        "        line = ''\r\n",
        "        times = []\r\n",
        "        times2 = []\r\n",
        "        times3 = []\r\n",
        "        times4 = []\r\n",
        "        numlines+=1\r\n",
        "    line+=unichr(int(row[1])+ascii_offset)\r\n",
        "    timesincelastevent = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(lasteventtime))\r\n",
        "    timesincecasestart = datetime.fromtimestamp(time.mktime(t))-datetime.fromtimestamp(time.mktime(casestarttime))\r\n",
        "    midnight = datetime.fromtimestamp(time.mktime(t)).replace(hour=0, minute=0, second=0, microsecond=0)\r\n",
        "    timesincemidnight = datetime.fromtimestamp(time.mktime(t))-midnight\r\n",
        "    timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds\r\n",
        "    timediff2 = 86400 * timesincecasestart.days + timesincecasestart.seconds\r\n",
        "    timediff3 = timesincemidnight.seconds #this leaves only time even occured after midnight\r\n",
        "    timediff4 = datetime.fromtimestamp(time.mktime(t)).weekday() #day of the week\r\n",
        "    times.append(timediff)\r\n",
        "    times2.append(timediff2)\r\n",
        "    times3.append(timediff3)\r\n",
        "    times4.append(timediff4)\r\n",
        "    lasteventtime = t\r\n",
        "    firstLine = False\r\n",
        "\r\n",
        "# add last case\r\n",
        "lines.append(line)\r\n",
        "timeseqs.append(times)\r\n",
        "timeseqs2.append(times2)\r\n",
        "timeseqs3.append(times3)\r\n",
        "timeseqs4.append(times4)\r\n",
        "numlines+=1\r\n",
        "\r\n",
        "elems_per_fold = int(round(numlines/3))\r\n",
        "fold1 = lines[:elems_per_fold]\r\n",
        "fold1_t = timeseqs[:elems_per_fold]\r\n",
        "fold1_t2 = timeseqs2[:elems_per_fold]\r\n",
        "fold1_t3 = timeseqs3[:elems_per_fold]\r\n",
        "fold1_t4 = timeseqs4[:elems_per_fold]\r\n",
        "with open('output_files/folds/fold1.csv', 'wb') as csvfile:\r\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\r\n",
        "    for row, timeseq in izip(fold1, fold1_t):\r\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\r\n",
        "\r\n",
        "fold2 = lines[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t = timeseqs[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t2 = timeseqs2[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t3 = timeseqs3[elems_per_fold:2*elems_per_fold]\r\n",
        "fold2_t4 = timeseqs4[elems_per_fold:2*elems_per_fold]\r\n",
        "with open('output_files/folds/fold2.csv', 'wb') as csvfile:\r\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\r\n",
        "    for row, timeseq in izip(fold2, fold2_t):\r\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\r\n",
        "\r\n",
        "fold3 = lines[2*elems_per_fold:]\r\n",
        "fold3_t = timeseqs[2*elems_per_fold:]\r\n",
        "fold3_t2 = timeseqs2[2*elems_per_fold:]\r\n",
        "fold3_t3 = timeseqs3[2*elems_per_fold:]\r\n",
        "fold3_t4 = timeseqs4[2*elems_per_fold:]\r\n",
        "with open('output_files/folds/fold3.csv', 'wb') as csvfile:\r\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\r\n",
        "    for row, timeseq in izip(fold3, fold3_t):\r\n",
        "        spamwriter.writerow([unicode(s).encode(\"utf-8\") +'#{}'.format(t) for s, t in izip(row, timeseq)])\r\n",
        "\r\n",
        "lines = fold1 + fold2\r\n",
        "lines_t = fold1_t + fold2_t\r\n",
        "lines_t2 = fold1_t2 + fold2_t2\r\n",
        "lines_t3 = fold1_t3 + fold2_t3\r\n",
        "lines_t4 = fold1_t4 + fold2_t4\r\n",
        "\r\n",
        "step = 1\r\n",
        "sentences = []\r\n",
        "softness = 0\r\n",
        "next_chars = []\r\n",
        "lines = map(lambda x: x+'!',lines)\r\n",
        "\r\n",
        "sentences_t = []\r\n",
        "sentences_t2 = []\r\n",
        "sentences_t3 = []\r\n",
        "sentences_t4 = []\r\n",
        "next_chars_t = []\r\n",
        "next_chars_t2 = []\r\n",
        "next_chars_t3 = []\r\n",
        "next_chars_t4 = []\r\n",
        "for line, line_t, line_t2, line_t3, line_t4 in izip(lines, lines_t, lines_t2, lines_t3, lines_t4):\r\n",
        "    for i in range(0, len(line), step):\r\n",
        "        if i==0:\r\n",
        "            continue\r\n",
        "\r\n",
        "        #we add iteratively, first symbol of the line, then two first, three...\r\n",
        "\r\n",
        "        sentences.append(line[0: i])\r\n",
        "        sentences_t.append(line_t[0:i])\r\n",
        "        sentences_t2.append(line_t2[0:i])\r\n",
        "        sentences_t3.append(line_t3[0:i])\r\n",
        "        sentences_t4.append(line_t4[0:i])\r\n",
        "        next_chars.append(line[i])\r\n",
        "        if i==len(line)-1: # special case to deal time of end character\r\n",
        "            next_chars_t.append(0)\r\n",
        "            next_chars_t2.append(0)\r\n",
        "            next_chars_t3.append(0)\r\n",
        "            next_chars_t4.append(0)\r\n",
        "        else:\r\n",
        "            next_chars_t.append(line_t[i])\r\n",
        "            next_chars_t2.append(line_t2[i])\r\n",
        "            next_chars_t3.append(line_t3[i])\r\n",
        "            next_chars_t4.append(line_t4[i])\r\n",
        "print('nb sequences:', len(sentences))\r\n",
        "\r\n",
        "print('Vectorization...')\r\n",
        "num_features = len(chars)+5\r\n",
        "print('num features: {}'.format(num_features))\r\n",
        "X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)\r\n",
        "y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)\r\n",
        "y_t = np.zeros((len(sentences)), dtype=np.float32)\r\n",
        "for i, sentence in enumerate(sentences):\r\n",
        "    leftpad = maxlen-len(sentence)\r\n",
        "    next_t = next_chars_t[i]\r\n",
        "    sentence_t = sentences_t[i]\r\n",
        "    sentence_t2 = sentences_t2[i]\r\n",
        "    sentence_t3 = sentences_t3[i]\r\n",
        "    sentence_t4 = sentences_t4[i]\r\n",
        "    for t, char in enumerate(sentence):\r\n",
        "        multiset_abstraction = Counter(sentence[:t+1])\r\n",
        "        for c in chars:\r\n",
        "            if c==char: #this will encode present events to the right places\r\n",
        "                X[i, t+leftpad, char_indices[c]] = 1\r\n",
        "        X[i, t+leftpad, len(chars)] = t+1\r\n",
        "        X[i, t+leftpad, len(chars)+1] = sentence_t[t]/divisor\r\n",
        "        X[i, t+leftpad, len(chars)+2] = sentence_t2[t]/divisor2\r\n",
        "        X[i, t+leftpad, len(chars)+3] = sentence_t3[t]/86400\r\n",
        "        X[i, t+leftpad, len(chars)+4] = sentence_t4[t]/7\r\n",
        "    for c in target_chars:\r\n",
        "        if c==next_chars[i]:\r\n",
        "            y_a[i, target_char_indices[c]] = 1-softness\r\n",
        "        else:\r\n",
        "            y_a[i, target_char_indices[c]] = softness/(len(target_chars)-1)\r\n",
        "    y_t[i] = next_t/divisor\r\n",
        "    np.set_printoptions(threshold=np.nan)\r\n",
        "\r\n",
        "# build the model: \r\n",
        "print('Build model...')\r\n",
        "main_input = Input(shape=(maxlen, num_features), name='main_input')\r\n",
        "# train a 2-layer LSTM with one shared layer\r\n",
        "l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer\r\n",
        "b1 = BatchNormalization()(l1)\r\n",
        "l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction\r\n",
        "b2_1 = BatchNormalization()(l2_1)\r\n",
        "l2_2 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in time prediction\r\n",
        "b2_2 = BatchNormalization()(l2_2)\r\n",
        "act_output = Dense(len(target_chars), activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)\r\n",
        "time_output = Dense(1, kernel_initializer='glorot_uniform', name='time_output')(b2_2)\r\n",
        "\r\n",
        "model = Model(inputs=[main_input], outputs=[act_output, time_output])\r\n",
        "\r\n",
        "opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\r\n",
        "\r\n",
        "model.compile(loss={'act_output':'categorical_crossentropy', 'time_output':'mae'}, optimizer=opt)\r\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=42)\r\n",
        "model_checkpoint = ModelCheckpoint('output_files/models/model_{epoch:02d}-{val_loss:.2f}.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')\r\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\r\n",
        "\r\n",
        "model.fit(X, {'act_output':y_a, 'time_output':y_t}, validation_split=0.2, verbose=2, callbacks=[early_stopping, model_checkpoint, lr_reducer], batch_size=maxlen, epochs=500)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-07e069b51436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mcsvfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meventlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mspamreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspamreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# skip the headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/helpdesk.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJBckG3Sbtmk",
        "outputId": "c87c35eb-6901-46b7-8d6a-4364c1023f6c"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xqe1SOfYyNp"
      },
      "source": [
        "# New Section"
      ]
    }
  ]
}